{
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Credit Card Fraud Detection with Scikit-Learn\n",
      "\n",
      "This notebook implements logistic regression using scikit-learn for credit card fraud detection, with the following improvements:\n",
      "1. Learning curve analysis to assess model generalization\n",
      "2. Using AUPRC instead of ROC-AUC for imbalanced data\n",
      "3. L2 regularization to improve generalization\n",
      "4. Optimized prediction threshold and train-test split ratio"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Import Libraries"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "from IPython.display import display\n",
      "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import (confusion_matrix, roc_curve, roc_auc_score, \n",
      "                           classification_report, average_precision_score, \n",
      "                           precision_recall_curve, f1_score, precision_score, recall_score)\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# Set random state and other parameters for reproducibility\n",
      "rState = 42\n",
      "test_size = 0.3  # We will test different test sizes for better performance\n",
      "prediction_threshold = 0.4  # We will test different thresholds for better performance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load and Explore Data"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Load the dataset\n",
      "df = pd.read_csv(\"fraud_data.csv\")\n",
      "\n",
      "# Display the first few rows of the dataset\n",
      "print(\"First few rows of the dataset:\")\n",
      "display(df.head())"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Display dataset information\n",
      "print(\"Dataset information:\")\n",
      "df.info()"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Check class distribution\n",
      "print(\"Class distribution:\")\n",
      "print(df['is_fraud'].value_counts())"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Preprocessing"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Dropping the two observations that are not a 0 or 1\n",
      "df = df[df['is_fraud'].isin(['1', '0'])]\n",
      "\n",
      "# Remove columns that aren't needed for the analysis\n",
      "df.drop(columns=['trans_num', 'city', 'state', 'job', 'merchant'], inplace=True)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Engineering"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Convert transaction date to datetime\n",
      "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], format=\"%d-%m-%Y %H:%M\")\n",
      "\n",
      "# Calculate age from date of birth\n",
      "df['age'] = (df['trans_date_trans_time'] - pd.to_datetime(df['dob'], format=\"%d-%m-%Y\")).dt.days // 365\n",
      "\n",
      "# Extract hour of day\n",
      "df['hour'] = df['trans_date_trans_time'].dt.hour\n",
      "\n",
      "# Extract day of the week\n",
      "df['day_of_week'] = df['trans_date_trans_time'].dt.day_name()"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Define the Haversine function to calculate distance\n",
      "def haversine(lat1, lon1, lat2, lon2):\n",
      "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
      "    dlat = lat2 - lat1\n",
      "    dlon = lon2 - lon1\n",
      "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
      "    c = 2 * math.asin(math.sqrt(a))\n",
      "    r = 6371.0  # Radius of Earth in kilometers\n",
      "    distance = r * c\n",
      "    return distance\n",
      "    \n",
      "# Calculate distance between customer and merchant\n",
      "df['distance_km'] = df.apply(\n",
      "    lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']),\n",
      "    axis=1\n",
      ")"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Convert categorical variables\n",
      "df['category'] = df['category'].astype('category')\n",
      "df['day_of_week'] = df['day_of_week'].astype('category')\n",
      "df['hour'] = df['hour'].astype('category')\n",
      "df['is_fraud'] = df['is_fraud'].astype(int)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Check the processed dataframe\n",
      "df.head()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploratory Data Analysis"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Display correlation matrix for numeric data\n",
      "quantdata = df[[\"amt\", \"city_pop\", \"age\", \"distance_km\"]]\n",
      "corr_matrix = quantdata.corr()\n",
      "plt.figure(figsize=(16, 12))\n",
      "sns.heatmap(corr_matrix, annot=True, cmap='Blues')\n",
      "plt.title(\"Correlation Heatmap\")\n",
      "plt.show()"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Check for missing values\n",
      "nan_counts = df.isna().sum()\n",
      "print(\"Missing values in each column:\")\n",
      "print(nan_counts)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model Preparation"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Keep only needed columns for the model\n",
      "model_df = df[['is_fraud', 'category', 'amt', 'city_pop', 'age', 'hour', 'day_of_week', 'distance_km']]\n",
      "\n",
      "# Split data into features and target\n",
      "X = model_df.drop('is_fraud', axis=1)\n",
      "y = model_df['is_fraud']\n",
      "\n",
      "# Split into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rState)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Identify categorical and numerical columns\n",
      "categorical_columns = ['category', 'hour', 'day_of_week']\n",
      "numeric_columns = ['amt', 'city_pop', 'age', 'distance_km']\n",
      "\n",
      "# Create preprocessor\n",
      "preprocessor = ColumnTransformer(\n",
      "    transformers=[\n",
      "        ('num', StandardScaler(), numeric_columns),\n",
      "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_columns)\n",
      "    ])"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Base Logistic Regression Model"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Build base Logistic Regression pipeline\n",
      "log_reg_pipeline = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('classifier', LogisticRegression(max_iter=1000, random_state=rState))\n",
      "])\n",
      "\n",
      "# Fit the model\n",
      "log_reg_pipeline.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_prob = log_reg_pipeline.predict_proba(X_test)[:, 1]\n",
      "y_pred = (y_prob > prediction_threshold).astype(int)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Calculate evaluation metrics\n",
      "naive_accuracy = 1 - y_test.sum() / len(y_test)\n",
      "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
      "auprc = average_precision_score(y_test, y_prob)\n",
      "auroc = roc_auc_score(y_test, y_prob)\n",
      "f1 = f1_score(y_test, y_pred)\n",
      "precision = precision_score(y_test, y_pred)\n",
      "recall = recall_score(y_test, y_pred)\n",
      "\n",
      "# Print evaluation metrics\n",
      "print(\"Evaluation Metrics for Base Logistic Regression:\")\n",
      "print(f\"Naive Accuracy: {naive_accuracy:.4f}\")\n",
      "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
      "print(f\"AUROC: {auroc:.4f}\")\n",
      "print(f\"AUPRC: {auprc:.4f}\")\n",
      "print(f\"F1 Score: {f1:.4f}\")\n",
      "print(f\"Precision: {precision:.4f}\")\n",
      "print(f\"Recall: {recall:.4f}\")"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Compute and plot confusion matrix\n",
      "cm = confusion_matrix(y_test, y_pred, normalize=\"true\")  # Normalized confusion matrix\n",
      "labels = [\"Not Fraud\", \"Fraud\"]\n",
      "\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(cm, annot=True, fmt='.2%', cmap='Blues', cbar=False,\n",
      "            xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 12})\n",
      "plt.xlabel('Predicted Labels')\n",
      "plt.ylabel('True Labels')\n",
      "plt.title('Confusion Matrix for Base Logistic Regression')\n",
      "plt.show()\n",
      "\n",
      "# Print classification report\n",
      "print(\"\\nClassification Report for Base Logistic Regression:\")\n",
      "print(classification_report(y_test, y_pred))"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Plot precision-recall curve\n",
      "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
      "plt.figure(figsize=(8, 6))\n",
      "plt.plot(recall, precision, color='b', label=f'AUPRC = {auprc:.3f}')\n",
      "\n",
      "# Add a line for random performance (class imbalance ratio)\n",
      "no_skill = y_test.sum() / len(y_test)\n",
      "plt.plot([0, 1], [no_skill, no_skill], 'r--', label=f'Random (AUPRC = {no_skill:.3f})')\n",
      "\n",
      "plt.xlabel('Recall')\n",
      "plt.ylabel('Precision')\n",
      "plt.title('Precision-Recall Curve for Base Logistic Regression')\n",
      "plt.legend(loc='best')\n",
      "plt.grid(True)\n",
      "plt.show()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Learning Curve Analysis"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "print(\"Learning Curve Analysis for Logistic Regression\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Calculate learning curve using F1 score since dataset is imbalanced\n",
      "train_sizes, train_scores, valid_scores = learning_curve(\n",
      "    log_reg_pipeline, X, y, \n",
      "    train_sizes=np.linspace(0.1, 1.0, 10),  # Train on 10%, 20%, ..., 100% of data\n",
      "    cv=5,  # 5-fold cross-validation\n",
      "    scoring='f1',  # Using F1 score for imbalanced data\n",
      "    n_jobs=-1  # Use all available cores\n",
      ")\n",
      "\n",
      "# Calculate mean and standard deviation of training and validation scores\n",
      "train_mean = np.mean(train_scores, axis=1)\n",
      "train_std = np.std(train_scores, axis=1)\n",
      "valid_mean = np.mean(valid_scores, axis=1)\n",
      "valid_std = np.std(valid_scores, axis=1)\n",
      "\n",
      "# Plot the learning curve\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.title(\"Learning Curve for Logistic Regression (F1 Score)\")\n",
      "plt.xlabel(\"Training Examples\")\n",
      "plt.ylabel(\"F1 Score\")\n",
      "plt.grid()\n",
      "\n",
      "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\n",
      "plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, alpha=0.1, color=\"red\")\n",
      "plt.plot(train_sizes, train_mean, 'o-', color=\"blue\", label=\"Training score\")\n",
      "plt.plot(train_sizes, valid_mean, 'o-', color=\"red\", label=\"Cross-validation score\")\n",
      "\n",
      "plt.legend(loc=\"best\")\n",
      "plt.show()\n",
      "\n",
      "# Print numeric results\n",
      "print(\"Training examples\\tTraining Score\\tValidation Score\")\n",
      "for i in range(len(train_sizes)):\n",
      "    print(f\"{train_sizes[i]:.0f}\\t\\t{train_mean[i]:.4f}\\t\\t{valid_mean[i]:.4f}\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. L2 Regularization"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "print(\"L2 Regularization for Logistic Regression\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Test different C values (inverse of regularization strength)\n",
      "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
      "results = []\n",
      "\n",
      "for C in C_values:\n",
      "    # Create logistic regression with L2 regularization\n",
      "    log_reg_l2 = Pipeline([\n",
      "        ('preprocessor', preprocessor),\n",
      "        ('classifier', LogisticRegression(C=C, penalty='l2', max_iter=1000, \n",
      "                                         random_state=rState, class_weight='balanced'))\n",
      "    ])\n",
      "    \n",
      "    # Train the model\n",
      "    log_reg_l2.fit(X_train, y_train)\n",
      "    \n",
      "    # Make predictions\n",
      "    y_prob_l2 = log_reg_l2.predict_proba(X_test)[:, 1]\n",
      "    y_pred_l2 = (y_prob_l2 > prediction_threshold).astype(int)\n",
      "    \n",
      "    # Calculate metrics\n",
      "    auprc_l2 = average_precision_score(y_test, y_prob_l2)\n",
      "    f1_l2 = f1_score(y_test, y_pred_l2)\n",
      "    recall_l2 = recall_score(y_test, y_pred_l2)\n",
      "    precision_l2 = precision_score(y_test, y_pred_l2)\n",
      "    \n",
      "    # Store results\n",
      "    results.append({\n",
      "        'C': C,\n",
      "        'AUPRC': auprc_l2,\n",
      "        'F1': f1_l2,\n",
      "        'Recall': recall_l2,\n",
      "        'Precision': precision_l2\n",
      "    })\n",
      "\n",
      "# Convert results to DataFrame\n",
      "results_df = pd.DataFrame(results)\n",
      "\n",
      "# Print results\n",
      "print(\"L2 Regularization Results:\")\n",
      "print(results_df)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Plot results\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.semilogx(results_df['C'], results_df['AUPRC'], 'o-', label='AUPRC')\n",
      "plt.semilogx(results_df['C'], results_df['F1'], 'o-', label='F1')\n",
      "plt.semilogx(results_df['C'], results_df['Recall'], 'o-', label='Recall')\n",
      "plt.semilogx(results_df['C'], results_df['Precision'], 'o-', label='Precision')\n",
      "plt.xlabel('C (inverse of regularization strength)')\n",
      "plt.ylabel('Score')\n",
      "plt.title('L2 Regularization Effect on Model Performance')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "# Find the best C value based on AUPRC\n",
      "best_C = results_df.loc[results_df['AUPRC'].idxmax(), 'C']\n",
      "print(f\"\\nBest C value based on AUPRC: {best_C}\")\n",
      "\n",
      "# Create model with the best C value\n",
      "best_log_reg = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('classifier', LogisticRegression(C=best_C, penalty='l2', max_iter=1000, \n",
      "                                     random_state=rState, class_weight='balanced'))\n",
      "])\n",
      "\n",
      "# Train the model\n",
      "best_log_reg.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_prob_best = best_log_reg.predict_proba(X_test)[:, 1]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Prediction Threshold Optimization"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "print(\"Prediction Threshold Optimization\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Test different threshold values\n",
      "thresholds = np.arange(0.1, 0.91, 0.1)\n",
      "threshold_results = []\n",
      "\n",
      "for threshold in thresholds:\n",
      "    # Apply threshold\n",
      "    y_pred_threshold = (y_prob_best > threshold).astype(int)\n",
      "    \n",
      "    # Calculate metrics\n",
      "    f1_threshold = f1_score(y_test, y_pred_threshold)\n",
      "    recall_threshold = recall_score(y_test, y_pred_threshold)\n",
      "    precision_threshold = precision_score(y_test, y_pred_threshold)\n",
      "    \n",
      "    # Store results\n",
      "    threshold_results.append({\n",
      "        'Threshold': threshold,\n",
      "        'F1': f1_threshold,\n",
      "        'Recall': recall_threshold,\n",
      "        'Precision': precision_threshold\n",
      "    })\n",
      "\n",
      "# Convert results to DataFrame\n",
      "threshold_df = pd.DataFrame(threshold_results)\n",
      "\n",
      "# Print results\n",
      "print(\"Prediction Threshold Results:\")\n",
      "print(threshold_df)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Plot results\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.plot(threshold_df['Threshold'], threshold_df['F1'], 'o-', label='F1')\n",
      "plt.plot(threshold_df['Threshold'], threshold_df['Recall'], 'o-', label='Recall')\n",
      "plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 'o-', label='Precision')\n",
      "plt.xlabel('Threshold')\n",
      "plt.ylabel('Score')\n",
      "plt.title('Effect of Prediction Threshold on Model Performance')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "# Find the best threshold based on F1 score\n",
      "best_threshold = threshold_df.loc[threshold_df['F1'].idxmax(), 'Threshold']\n",
      "print(f\"\\nBest threshold based on F1 score: {best_threshold}\")\n",
      "\n",
      "# Apply the best threshold\n",
      "y_pred_best = (y_prob_best > best_threshold).astype(int)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# Calculate final metrics\n",
      "final_auprc = average_precision_score(y_test, y_prob_best)\n",
      "final_f1 = f1_score(y_test, y_pred_best)\n",
      "final_recall = recall_score(y_test, y_pred_best)\n",
      "final_precision = precision_score(y_test, y_pred_best)\n",
      "\n",
      "# Print final metrics\n",
      "print(\"Final Model Metrics (with L2 Regularization and Optimal Threshold):\")\n",
      "print(f\"AUPRC: {final_auprc:.4f}\")\n",
      "print(f\"F1 Score: {final_f1:.4f}\")\n",
      "print(f\"Recall: {final_recall:.4f}\")\n",
      "print(f\"Precision: {final_precision:.4f}\")\n",
      "\n",
      "# Compute and plot final confusion matrix\n",
      "final_cm = confusion_matrix(y_test, y_pred_best, normalize=\"true\")\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(final_cm, annot=True, fmt='.2%', cmap='Blues', cbar=False,\n",
      "            xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 12})\n",
      "plt.xlabel('Predicted Labels')\n",
      "plt.ylabel('True Labels')\n",
      "plt.title('Final Confusion Matrix (with L2 Regularization and Optimal Threshold)')\n",
      "plt.show()\n",
      "\n",
      "# Print final classification report\n",
      "print(\"\\nFinal Classification Report:\")\n",
      "print(classification_report(y_test, y_pred_best))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5. Train-Test Split Ratio Comparison"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "print(\"Train-Test Split Ratio Comparison\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Test different train-test split ratios\n",
      "test_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
      "split_results = []\n",
      "\n",
      "for test_ratio in test_sizes:\n",
      "    # Split the data\n",
      "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
      "        X, y, test_size=test_ratio, random_state=rState)\n",
      "    \n",
      "    # Create and train the model\n",
      "    log_reg_split = Pipeline([\n",
      "        ('preprocessor', preprocessor),\n",
      "        ('classifier', LogisticRegression(C=best_C, penalty='l2', max_iter=1000, \n",
      "                                         random_state=rState, class_weight='balanced'))\n",
      "    ])\n",
      "    log_reg_split.fit(X_train_split, y_train_split)\n",
      "    \n",
      "    # Make predictions\n",
      "    y_prob_split = log_reg_split.predict_proba(X_test_split)[:, 1]\n",
      "    y_pred_split = (y_prob_split > best_threshold).astype(int)\n",
      "    \n",
      "    # Calculate metrics\n",
      "    auprc_split = average_precision_score(y_test_split, y_prob_split)\n",
      "    f1_split = f1_score(y_test_split, y_pred_split)\n",
      "    \n",
      "    # Store results\n",
      "    split_results.append({\n",
      "        'Test Size': test_ratio,\n",
      "        'Training Size': 1 - test_ratio,\n",
      "        'AUPRC': auprc_split,\n",
      "        'F1': f1_split\n",
      "    })\n",
      "\n",
      "# Convert results to DataFrame\n",
      "split_df = pd.DataFrame(split_results)\n",
      "\n",
      "# Print results\n",
      "print(\"Train-Test Split Ratio Results:\")\n",
      "print(split_df)\n",
      "\n",
      "# Plot results\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.plot(split_df['Training Size'], split_df['AUPRC'], 'o-', label='AUPRC')\n",
      "plt.plot(split_df['Training Size'], split_df['F1'], 'o-', label='F1')\n",
      "plt.xlabel('Training Data Size Ratio')\n",
      "plt.ylabel('Score')\n",
      "plt.title('Effect of Train-Test Split Ratio on Model Performance')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "# Find the best split ratio based on AUPRC\n",
      "best_training_size = split_df.loc[split_df['AUPRC'].idxmax(), 'Training Size']\n",
      "best_test_size = 1 - best_training_size\n",
      "print(f\"\\nBest train-test split ratio based on AUPRC: {best_training_size:.1f}/{best_test_size:.1f}\")\n",
      "\n",
      "# 6. Feature Importance Analysis for the final model\n",
      "print(\"\\n\" + \"=\"*50)\n",
      "print(\"Feature Importance Analysis\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Get feature names after preprocessing\n",
      "preprocessor.fit(X)\n",
      "num_features = preprocessor.transformers_[0][1].get_feature_names_out(numeric_columns)\n",
      "cat_features = preprocessor.transformers_[1][1].get_feature_names_out(categorical_columns)\n",
      "feature_names = np.concatenate([num_features, cat_features])\n",
      "\n",
      "# Train the final model\n",
      "final_log_reg = LogisticRegression(C=best_C, penalty='l2', max_iter=1000, \n",
      "                                  random_state=rState, class_weight='balanced')\n",
      "X_preprocessed = preprocessor.transform(X_train)\n",
      "final_log_reg.fit(X_preprocessed, y_train)\n",
      "\n",
      "# Get feature coefficients\n",
      "coefficients = final_log_reg.coef_[0]\n",
      "\n",
      "# Create DataFrame with feature names and coefficients\n",
      "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
      "feature_importance['Absolute Coefficient'] = abs(feature_importance['Coefficient'])\n",
      "feature_importance = feature_importance.sort_values('Absolute Coefficient', ascending=False)\n",
      "\n",
      "# Plot feature importance\n",
      "plt.figure(figsize=(14, 8))\n",
      "plt.barh(feature_importance['Feature'][:15], feature_importance['Coefficient'][:15])\n",
      "plt.xlabel('Coefficient')\n",
      "plt.ylabel('Feature')\n",
      "plt.title('Top 15 Features by Importance')\n",
      "plt.grid(True, axis='x')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nTop 15 features by importance:\")\n",
      "print(feature_importance.head(15))\n",
      "\n",
      "# 7. Random Forest model for comparison\n",
      "print(\"\\n\" + \"=\"*50)\n",
      "print(\"Random Forest Model for Comparison\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Create Random Forest pipeline\n",
      "rf_pipeline = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=rState))\n",
      "])\n",
      "\n",
      "# Train the model\n",
      "rf_pipeline.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_prob_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
      "y_pred_rf = (y_prob_rf > best_threshold).astype(int)  # Use the same optimized threshold\n",
      "\n",
      "# Calculate metrics\n",
      "auprc_rf = average_precision_score(y_test, y_prob_rf)\n",
      "f1_rf = f1_score(y_test, y_pred_rf)\n",
      "recall_rf = recall_score(y_test, y_pred_rf)\n",
      "precision_rf = precision_score(y_test, y_pred_rf)\n",
      "\n",
      "# Print metrics\n",
      "print(\"Random Forest Model Metrics:\")\n",
      "print(f\"AUPRC: {auprc_rf:.4f}\")\n",
      "print(f\"F1 Score: {f1_rf:.4f}\")\n",
      "print(f\"Recall: {recall_rf:.4f}\")\n",
      "print(f\"Precision: {precision_rf:.4f}\")\n",
      "\n",
      "# Compute and plot confusion matrix\n",
      "cm_rf = confusion_matrix(y_test, y_pred_rf, normalize=\"true\")\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(cm_rf, annot=True, fmt='.2%', cmap='Blues', cbar=False,\n",
      "            xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 12})\n",
      "plt.xlabel('Predicted Labels')\n",
      "plt.ylabel('True Labels')\n",
      "plt.title('Confusion Matrix for Random Forest')\n",
      "plt.show()\n",
      "\n",
      "# Print classification report\n",
      "print(\"\\nRandom Forest Classification Report:\")\n",
      "print(classification_report(y_test, y_pred_rf))\n",
      "\n",
      "# 8. Model Comparison\n",
      "print(\"\\n\" + \"=\"*50)\n",
      "print(\"Model Comparison\")\n",
      "print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "# Create comparison DataFrame\n",
      "models = ['Base Logistic Regression', 'Optimized Logistic Regression', 'Random Forest']\n",
      "metrics = ['AUPRC', 'F1 Score', 'Recall', 'Precision']\n",
      "\n",
      "comparison_data = {\n",
      "    'AUPRC': [auprc, final_auprc, auprc_rf],\n",
      "    'F1 Score': [f1, final_f1, f1_rf],\n",
      "    'Recall': [recall, final_recall, recall_rf],\n",
      "    'Precision': [precision, final_precision, precision_rf]\n",
      "}\n",
      "\n",
      "comparison_df = pd.DataFrame(comparison_data, index=models)\n",
      "\n",
      "# Print comparison\n",
      "print(\"Model Performance Comparison:\")\n",
      "print(comparison_df)\n",
      "\n",
      "# Plot comparison\n",
      "plt.figure(figsize=(12, 8))\n",
      "comparison_df.plot(kind='bar', figsize=(12, 8))\n",
      "plt.title('Model Performance Comparison')\n",
      "plt.ylabel('Score')\n",
      "plt.xlabel('Model')\n",
      "plt.xticks(rotation=45)\n",
      "plt.legend(loc='best')\n",
      "plt.grid(True, axis='y')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nConclusion:\")\n",
      "print(\"The optimized logistic regression model with L2 regularization (C={:.3f}) and\".format(best_C))\n",
      "print(\"prediction threshold of {:.1f} achieves the best balance of precision and recall.\".format(best_threshold))\n",
      "print(\"This implementation demonstrates the improvements we proposed in the project document:\")\n",
      "print(\"1. Learning curve analysis to assess model generalization\")\n",
      "print(\"2. Using AUPRC instead of ROC-AUC for imbalanced data\")\n",
      "print(\"3. L2 regularization to improve generalization\")\n",
      "print(\"4. Optimized prediction threshold and train-test split ratio\")"
     ]
    }
   ],
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.8.10"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 4
  }